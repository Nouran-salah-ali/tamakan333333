//
//  AudioRecordingViewModel.swift
//  voiceTamakan
//
//  Created by Nouran Salah
//

import AVFoundation
import Combine
import WhisperKit
import SwiftData
import SwiftUI

class AudioRecordingViewModel: ObservableObject {

    // MARK: - Published UI variables
    @Published var finalText: String = ""

    // MARK: - Audio
    private let audioEngine = AVAudioEngine()
    private var audioFile: AVAudioFile?
    private var player: AVAudioPlayer?
    private var lastRecordingURL: URL?

    private var bufferQueue: [AVAudioPCMBuffer] = []

    // MARK: - Whisper model (single instance)
    private var whisper: WhisperKit?

    
    
   // @Environment(\.presentationMode) var presentationMode
   // @Environment(\.modelContext) private var context
    var context: ModelContext?
    // MARK: - Init (load model only once)
    init() {
        Task {
            do {
                print("â³ Loading Whisper model...")
                //whisper = try await WhisperKit(WhisperKitConfig(model: "large"))
                whisper = try await WhisperKit(WhisperKitConfig(model: "medium"))
                print("âœ… Whisper model loaded successfully")
            } catch {
                print("âŒ Whisper init error:", error.localizedDescription)
            }
        }
    }


    // MARK: - Start Recording
    func startRecording() {

        // Request microphone and start session
        let session = AVAudioSession.sharedInstance()
        do {
            try session.setCategory(.playAndRecord, mode: .default)
            try session.setActive(true)
            print("ðŸŽ™ï¸ Session READY")
        } catch {
            print("âŒ Session error:", error)
        }

        let inputNode = audioEngine.inputNode
        let format = inputNode.inputFormat(forBus: 0)

        let timestamp = Date().timeIntervalSince1970
        let fileName = "recording_\(timestamp).caf"
        let url = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
            .appendingPathComponent(fileName)
        lastRecordingURL = url
        //i created func that create object object
        //save object in context
        addRecord(RcordName :fileName, duration: 0.0 ,date: Date(), finalText: finalText ,url: url)
        

        do {
            audioFile = try AVAudioFile(forWriting: url, settings: format.settings)
            print("ðŸ“ File created: \(fileName)")
        } catch {
            print("âŒ File creation error:", error)
        }

        bufferQueue.removeAll()
        inputNode.removeTap(onBus: 0)

        // Install audio tap
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: format) { [weak self] buffer, _ in
            guard let self = self else { return }

            
            // Detect blocking BEFORE doing anything else
            if self.detectBlocking(in: buffer, format: format) {
                print("â›” BLOCKING DETECTED (silent gap)")
            }
            // Write buffer to main recording file
            do {
                try self.audioFile?.write(from: buffer)
            } catch {
                print("âŒ Write error:", error)
            }

            self.bufferQueue.append(buffer)
            print("ðŸŽ§ Captured audio: \(buffer.frameLength)")

            // Every ~2â€“3 seconds â†’ process chunk
            let requiredFrames = AVAudioFrameCount(format.sampleRate * 2.5)
            let totalFrames = self.bufferQueue.reduce(0) { $0 + $1.frameLength }

            if totalFrames >= requiredFrames {
                let chunk = self.mergeBuffers(self.bufferQueue, format: format)
                self.bufferQueue.removeAll()

                let tempURL = self.saveBufferToFile(chunk)
                self.transcribeChunk(at: tempURL)
            }
        }

        // Start engine
        do {
            try audioEngine.start()
            print("â–¶ï¸ Engine started")
        } catch {
            print("âŒ Engine start error:", error)
        }
    }


    // MARK: - Stop Recording
    func stopRecording() {
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        print("â¹ï¸ Engine stopped")
    }


    // MARK: - Play last recording
    func playRecording() {
        guard let url = lastRecordingURL else {
            print("âš ï¸ No recording found")
            return
        }
        do {
            player = try AVAudioPlayer(contentsOf: url)
            player?.play()
            print("ðŸ”Š Playing recording")
        } catch {
            print("âŒ Playback error:", error)
        }
    }


    // MARK: - Transcription
    private func transcribeChunk(at url: URL) {
        Task { [weak self] in
            guard let self = self, let whisper = self.whisper else {
                print("âš ï¸ Whisper model not ready")
                return
            }

            do {
                print("â³ Transcribing chunk...")

                let results = try await whisper.transcribe(audioPath: url.path)
                let text = results.first?.text ?? ""

                DispatchQueue.main.async {
                    if !text.isEmpty {
                        self.finalText += text + " "
                        
                        // ðŸ‘‡ Detect stuttering
                        if self.analyzeStutter(text) {
                            print("ðŸŸ¥ Stuttering found in chunk")
                            // you can show alert / highlight / save event
                        } else {
                            print("ðŸŸ© No stuttering")
                        }
                        print("ðŸŸ¢ Transcribed:", text)
                    } else {
                        print("âš ï¸ Empty transcription")
                    }
                }

            } catch {
                print("âŒ Transcription error:", error.localizedDescription)
            }
        }
    }

    // MARK: - Buffer merge
    private func mergeBuffers(_ buffers: [AVAudioPCMBuffer], format: AVAudioFormat) -> AVAudioPCMBuffer {
        let totalFrames = buffers.reduce(0) { $0 + $1.frameLength }
        guard let merged = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: totalFrames) else {
            return buffers[0]
        }

        merged.frameLength = totalFrames

        var offset: AVAudioFrameCount = 0
        for buffer in buffers {
            for ch in 0..<Int(format.channelCount) {
                let src = buffer.floatChannelData![ch]
                let dst = merged.floatChannelData![ch]
                memcpy(dst.advanced(by: Int(offset)),
                       src,
                       Int(buffer.frameLength) * MemoryLayout<Float>.size)
            }
            offset += buffer.frameLength
        }
        return merged
    }


    // MARK: - Save buffer to temp file
    private func saveBufferToFile(_ buffer: AVAudioPCMBuffer) -> URL {
        let url = FileManager.default.temporaryDirectory.appendingPathComponent("chunk.caf")

        do {
            let file = try AVAudioFile(forWriting: url, settings: buffer.format.settings)
            try file.write(from: buffer)
            print("ðŸ“¦ Chunk saved:", url.path)
        } catch {
            print("âŒ Error saving chunk:", error)
        }

        return url
    }
    
    
    // MARK: - func save record in object

//    func addRecord(RcordName :String, duration: Double ,date: Date, finalText: String ,url: URL) {
//        let newRecord = RecordingModel(recordname: RcordName, duration: 0.0, date: date, transcript: finalText, audiofile: url)
//        //add record to arr
//        context.insert(newRecord)
//        print("ðŸ“¦ðŸ“¦ðŸ“¦ðŸ“¦ðŸ“¦ðŸ“¦OBJECT CREATED")
//    }
    
    // MARK: - Save new record
    func addRecord(RcordName: String, duration: Double, date: Date, finalText: String, url: URL) {

        guard let context else {
            print("âŒ ERROR: No ModelContext found.")
            return
        }

        let newRecord = RecordingModel(
            recordname: RcordName,
            duration: duration,
            date: date,
            transcript: finalText,
            audiofile: url
        )

        context.insert(newRecord)

        print("ðŸ“¦ Record saved:", RcordName)
    }
    
    func analyzeStutter(_ rawText: String) -> Bool {
        // 1ï¸âƒ£ Normalize text
        let text = rawText
            .lowercased()
            .replacingOccurrences(of: "[^a-zA-Z0-9\\s-]", with: "", options: .regularExpression) // remove punctuation
            .replacingOccurrences(of: "\\s+", with: " ", options: .regularExpression)            // collapse spaces

        // 2ï¸âƒ£ Patterns
        let soundPattern = #"\b([a-zA-Z])-\1-\1"#     // b-b-b
        let syllablePattern = #"\b([a-zA-Z]{1,})-\1"# // ta-ta
        let wordPattern = #"\b(\w+)\s+\1\s+\1"#       // hi hi hi

        let patterns = [soundPattern, syllablePattern, wordPattern]

        // 3ï¸âƒ£ Check patterns
        for pattern in patterns {
            if let _ = text.range(of: pattern, options: .regularExpression) {
                print("âš ï¸ STUTTER DETECTED:", rawText)
                return true
            }
        }

        return false
    }
    
    func detectBlocking(in buffer: AVAudioPCMBuffer, format: AVAudioFormat) -> Bool {
        let channel = buffer.floatChannelData![0]
        let frameCount = Int(buffer.frameLength)

        let silenceThreshold: Float = 0.0005   // detect near-silence
        var silentSamples = 0

        for i in 0..<frameCount {
            if abs(channel[i]) < silenceThreshold {
                silentSamples += 1
            }
        }

        let duration = Double(silentSamples) / format.sampleRate

        // If silence > 0.14 seconds inside speech â†’ possible block
        return duration > 0.14
    }
}
